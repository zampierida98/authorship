{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESECUZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec 28 17:45:00 2020\n",
    "\n",
    "Modulo per calcolare le varie metriche/attributi necessari\n",
    "per stabilire se un testo Ã¨ stato scritto da un autore\n",
    "\n",
    "@author: michele\n",
    "\"\"\"\n",
    "'''import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')'''\n",
    "\n",
    "def prob_of_comma(RDD_sentences_data, text_length):\n",
    "    return (RDD_sentences_data.flatMap(lambda x: x)\n",
    "           .filter(lambda x: \",\" in x)\n",
    "           .count()\n",
    "           ) / text_length\n",
    "\n",
    "def prob_distr_of_sentence_length(RDD_sen_len):\n",
    "    tot = len(getCollection(RDD_sen_len))\n",
    "    \n",
    "    return (RDD_sen_len.map(lambda x: (x,1))\n",
    "            .reduceByKey(lambda a,b: a+b)\n",
    "            .map(lambda x: (x[0], x[1]/tot))\n",
    "            .sortBy(lambda x: -x[1])\n",
    "           )\n",
    "\n",
    "def prob_of_the_most_freq_sentence_len(RDD_sen_len):\n",
    "    return prob_distr_of_sentence_length(RDD_sen_len).take(1)\n",
    "\n",
    "def sentence_lengths(RDD):\n",
    "    # OPERAZIONI PRELIMINARI SUL TESTO\n",
    "    text = RDD.flatMap(lambda x: x).reduce(lambda a,b: a + ' ' + b) # METTO TUTTO IL TESTO IN UNA STRINGA UNICA\n",
    "    text = text.replace(\"?\", \".\") # ? TERMINA UNA FRASE\n",
    "    text = text.replace(\"!\", \".\") # ! TERMINA UNA FRASE\n",
    "    text = text.split('. ') # SPLITTO QUANDO TROVO UN CARATTERE CHE TERMINA UNA FRASE (. SEGUITO DA UNO SPAZIO)\n",
    "    \n",
    "    return (sc.parallelize(text)\n",
    "            .map(lambda x: len(x.split(' ')))\n",
    "            .collect()\n",
    "           ) # PER OGNI FRASE TROVATA CONTO LE SUE PAROLE\n",
    "\n",
    "def prob_of_the_most_common_word_x(RDD_word_counter, text_len):\n",
    "    prep_art = open(\"preposizioni_e_articoli.txt\").read().splitlines()\n",
    "    \n",
    "    return sc.parallelize(prob_distr_of_30_most_common_words(RDD_word_counter, text_len)\n",
    "                          .filter(lambda x: x[0] not in prep_art)\n",
    "                          .take(1)\n",
    "                         )\n",
    "\n",
    "def prob_distr_of_30_most_common_words_x(RDD_word_counter, text_len):\n",
    "    \n",
    "    prep_art = open(\"preposizioni_e_articoli.txt\").read().splitlines()\n",
    "    \n",
    "    # probability distribution\n",
    "    return sc.parallelize(RDD_word_counter.filter(lambda x: x[0] not in prep_art)\n",
    "                          .map(lambda x: (x[0], x[1]/text_len))\n",
    "                          .take(30)\n",
    "                         )\n",
    "\n",
    "def avg_distance_consec_appear(RDD, MCW):\n",
    "    vect_dis = distance_consec_appear(RDD, MCW)\n",
    "    \n",
    "    _sum = 0\n",
    "    for i in vect_dis:\n",
    "        _sum += i\n",
    "    \n",
    "    return _sum/len(vect_dis)\n",
    "\n",
    "def max_distance_consec_appear(RDD, MCW):\n",
    "    return max(distance_consec_appear(RDD, MCW))\n",
    "\n",
    "def min_distance_consec_appear(RDD, MCW):\n",
    "    return min(distance_consec_appear(RDD, MCW))\n",
    "\n",
    "def distance_consec_appear(RDD, MCW):\n",
    "    if MCW == \",\":\n",
    "        vect_pos = (RDD.flatMap(lambda x:x)\n",
    "                    .zipWithIndex()\n",
    "                    .filter(lambda x: \",\" in x[0])\n",
    "                    .map(lambda x: [x[1]])\n",
    "                    .reduce(lambda a,b: a+b)\n",
    "                   )\n",
    "    else:\n",
    "        vect_pos = (RDD.flatMap(lambda x:x)\n",
    "                    .zipWithIndex()\n",
    "                    .filter(lambda x:x[0] == MCW)\n",
    "                    .map(lambda x: [x[1]])\n",
    "                    .reduce(lambda a,b: a+b)\n",
    "                   )\n",
    "    \n",
    "    vect_dis = []\n",
    "    \n",
    "    for i in range(1, len(vect_pos)):\n",
    "        vect_dis.append(vect_pos[i] - vect_pos[i-1])\n",
    "    \n",
    "    return vect_dis\n",
    "\n",
    "\n",
    "def prob_of_the_most_common_word(RDD_word_counter, text_len):\n",
    "    return sc.parallelize(prob_distr_of_30_most_common_words(RDD_word_counter, text_len)\n",
    "                          .filter(lambda x: x[0] != \"and\" and x[0] != \"the\")\n",
    "                          .take(1)\n",
    "                         )\n",
    "\n",
    "def prob_of_The(RDD_word_counter, text_len):\n",
    "    return (RDD_word_counter.filter(lambda x: x[0] == \"the\")\n",
    "           .map(lambda x: (x[0], x[1]/text_len))\n",
    "           )\n",
    "\n",
    "def prob_distr_of_30_most_common_words(RDD_word_counter, text_len):\n",
    "    # probability distribution\n",
    "    return sc.parallelize(RDD_word_counter.map(lambda x: (x[0], x[1]/text_len)).take(30))\n",
    "\n",
    "def hentropy(RDD_word_counter, text_len):\n",
    "    import math\n",
    "    \n",
    "    return (RDD_word_counter.map(lambda x: (\"hentropy\", (x[1]/text_len) * math.log2(x[1]/text_len)))\n",
    "                            .reduceByKey(lambda a,b: a+b)\n",
    "                            .map(lambda x: -x[1])    # entropia ha segno negativo\n",
    "           )\n",
    "\n",
    "def text_length_in_words(RDD_word_counter):\n",
    "    \n",
    "    # word_counter: [(\"word1\", 100), ...]\n",
    "    \n",
    "    return (RDD_word_counter.map(lambda x: x[1])\n",
    "                          .reduce(lambda a,b: a+b)\n",
    "           )\n",
    "\n",
    "def word_counter(RDD):\n",
    "    word_counter = (RDD.flatMap(lambda x: x)\n",
    "                .map(lambda x: (x,1))\n",
    "                .reduceByKey(lambda a,b: a+b)\n",
    "                .sortBy(lambda x: -x[1])\n",
    "               )\n",
    "    return word_counter, len(word_counter.collect())\n",
    "\n",
    "\n",
    "def getCollection(RDD):\n",
    "    return RDD.collect()\n",
    "\n",
    "def getValue(RDD):\n",
    "    return RDD.collect()[0]\n",
    "\n",
    "\n",
    "def remove_number_some_punctuation_marks(row):\n",
    "\n",
    "    lowercase = row.lower()\n",
    "    lowercase = lowercase.replace(\"--\", \" \")\n",
    "    \n",
    "    res = \"\"\n",
    "    \n",
    "    for char in lowercase:\n",
    "        if not ('0' <= char <= '9' or char == '\"'):\n",
    "            res += char\n",
    "\n",
    "    return res\n",
    "\n",
    "def remove_number_punctuation_marks(row):\n",
    "    \n",
    "    lowercase = row.lower()\n",
    "    lowercase = lowercase.replace(\"--\", \" \")\n",
    "    \n",
    "    res = \"\"\n",
    "    \n",
    "    for char in lowercase:\n",
    "        if 'a' <= char <= 'z' or char == ' ' or char == '-' or char == \"'\":\n",
    "            res += char\n",
    "\n",
    "    return res\n",
    "\n",
    "def load_file_without_punctuations_marks(filepath):\n",
    "    # caricamento del dataset\n",
    "    raw_text = sc.textFile(filepath)\n",
    "\n",
    "    # rimuoviamo i numeri e i segni di punteggiatura\n",
    "    \n",
    "    return (raw_text.filter(bool)                    # rimuoviamo le stringhe vuote\n",
    "        .map(remove_number_punctuation_marks)\n",
    "        .map(lambda x : ' '.join(x.split()))        # rimuoviamo diversi spazi bianchi con uno\n",
    "        .map(lambda row : row.split(\" \"))\n",
    "       )\n",
    "\n",
    "def load_file_without_number(filepath):\n",
    "    # caricamento del dataset\n",
    "    raw_text = sc.textFile(filepath)\n",
    "\n",
    "    # rimuoviamo i numeri e i segni di punteggiatura\n",
    "    \n",
    "    return (raw_text.filter(bool)                    # rimuoviamo le stringhe vuote\n",
    "        .map(remove_number_some_punctuation_marks)\n",
    "        .map(lambda x : ' '.join(x.split()))        # rimuoviamo diversi spazi bianchi con uno\n",
    "        .map(lambda row : row.split(\" \"))\n",
    "       )\n",
    "\n",
    "def generate_metrics(file_in):\n",
    "    res = {}\n",
    "    \n",
    "    print(\"Caricamento del file ... \", end=\" \")\n",
    "    data = load_file_without_punctuations_marks(file_in)\n",
    "    print(\"caricamento completato\")\n",
    "    \n",
    "    # POSIAMO I DATI NELLA CACHE\n",
    "    data.persist()\n",
    "    \n",
    "    print(\"Calcolo le metriche attendere ... \", end=\" \")\n",
    "    \n",
    "    RDD_word_counter, vocabulary_size = word_counter(data)\n",
    "    text_length = text_length_in_words(RDD_word_counter)\n",
    "    RDD_hentropy = hentropy(RDD_word_counter, text_length)\n",
    "\n",
    "    RDD_prob_distr_of_30 = prob_distr_of_30_most_common_words(RDD_word_counter, text_length)\n",
    "    RDD_prob_the_most_common_word = prob_of_the_most_common_word(RDD_word_counter, text_length)\n",
    "    RDD_prob_the = prob_of_The(RDD_word_counter, text_length)\n",
    "\n",
    "\n",
    "    avg_dist_consec_MCW = avg_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    min_dist_consec_MCW = min_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    max_dist_consec_MCW = max_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "\n",
    "    avg_dist_consec_the = avg_distance_consec_appear(data, \"the\")\n",
    "    min_dist_consec_the = min_distance_consec_appear(data, \"the\")\n",
    "    max_dist_consec_the = max_distance_consec_appear(data, \"the\")\n",
    "    \n",
    "    # genero MCWx da passare come parametro alle funzioni che calcolano le distanze\n",
    "    MCWx = getValue(prob_of_the_most_common_word_x(RDD_word_counter, text_length))[0]\n",
    "    \n",
    "    avg_dist_consec_MCWx = avg_distance_consec_appear(data, MCWx)\n",
    "    min_dist_consec_MCWx = min_distance_consec_appear(data, MCWx)\n",
    "    max_dist_consec_MCWx = max_distance_consec_appear(data, MCWx)\n",
    "\n",
    "    # CONSIDERIAMO IL TESTO CON I SEGNI DI PUNTEGGIATURA\n",
    "    \n",
    "    # genero la lista delle frasi del testo\n",
    "    sentences_data = load_file_without_number(file_in)\n",
    "    sentences_data.persist()\n",
    "    \n",
    "    #min(lengths), max(lengths), sum(lengths)/len(lengths)\n",
    "    sen_lengths = sentence_lengths(sentences_data)\n",
    "    \n",
    "    # trasporto in RDD\n",
    "    RDD_sen_lengths = sc.parallelize(sen_lengths)\n",
    "    RDD_sen_lengths.persist()\n",
    "    \n",
    "    # probabilitÃ  della lunghezza della frase piÃ¹ frequente\n",
    "    prob_most_freq_sen = prob_of_the_most_freq_sentence_len(RDD_sen_lengths)\n",
    "    \n",
    "    # distribuzione di probabilitÃ  sulla lunghezza delle frase \n",
    "    prob_distr_freq_sen = prob_distr_of_sentence_length(RDD_sen_lengths)\n",
    "    \n",
    "    # probabilitÃ  di incontrare una virgola\n",
    "    p_comma = prob_of_comma(sentences_data, text_length)\n",
    "\n",
    "    avg_dist_consec_comma = avg_distance_consec_appear(sentences_data, \",\")\n",
    "    min_dist_consec_comma = min_distance_consec_appear(sentences_data, \",\")\n",
    "    max_dist_consec_comma = max_distance_consec_appear(sentences_data, \",\")\n",
    "    \n",
    "    \n",
    "    # creiamo la struttura dati\n",
    "    res['vocabulary_size'] = vocabulary_size\n",
    "    res['text_length'] = text_length\n",
    "    res['V_T'] = vocabulary_size/text_length\n",
    "    res['hentropy'] = getValue(RDD_hentropy)\n",
    "    \n",
    "    res['prob_distr_of_30'] = getCollection(RDD_prob_distr_of_30)\n",
    "    res['prob_of_the_most_common_word'] = getValue(RDD_prob_the_most_common_word)[1]\n",
    "    res['prob_of_the'] = getValue(RDD_prob_the)[1]\n",
    "    res['prob_of_comma'] = p_comma\n",
    "    \n",
    "    res['avg_dist_consec_MCW'] = avg_dist_consec_MCW\n",
    "    res['min_dist_consec_MCW'] = min_dist_consec_MCW\n",
    "    res['max_dist_consec_MCW'] = max_dist_consec_MCW\n",
    "    \n",
    "    res['avg_dist_consec_the'] = avg_dist_consec_the\n",
    "    res['min_dist_consec_the'] = min_dist_consec_the\n",
    "    res['max_dist_consec_the'] = max_dist_consec_the\n",
    "    \n",
    "    res['avg_dist_consec_MCWx'] = avg_dist_consec_MCWx\n",
    "    res['min_dist_consec_MCWx'] = min_dist_consec_MCWx\n",
    "    res['max_dist_consec_MCWx'] = max_dist_consec_MCWx\n",
    "    \n",
    "    res['avg_dist_consec_comma'] = avg_dist_consec_comma\n",
    "    res['min_dist_consec_comma'] = min_dist_consec_comma\n",
    "    res['max_dist_consec_comma'] = max_dist_consec_comma\n",
    "    \n",
    "    res['avg_sentence_len'] = sum(sen_lengths)/len(sen_lengths)\n",
    "    res['max_sentence_len'] = max(sen_lengths)\n",
    "    res['min_sentence_len'] = min(sen_lengths)\n",
    "    \n",
    "    res['prob_most_freq_sen'] = prob_most_freq_sen[0][1]\n",
    "    res['prob_distr_freq_sen'] = getCollection(prob_distr_freq_sen)\n",
    "    \n",
    "    print(\"calcolo completato\")\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def save_metrics(file_in, file_out):\n",
    "    '''\n",
    "    save_metrics salva un dictionary in un file \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_in : str\n",
    "        path del file da analizzare\n",
    "    file_out: str\n",
    "        path del file in cui salvare la entry\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    '''\n",
    "    import pickle\n",
    "\n",
    "    entry = generate_metrics(file_in)\n",
    "    \n",
    "    with open(file_out, 'ab') as fout:\n",
    "        pickle.dump(entry, fout, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Salvataggio completato\")\n",
    "\n",
    "def load_metrics(file_in):\n",
    "    '''\n",
    "    load_metrics dal file le diverse righe (dict()) \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_in : str\n",
    "        path del file da caricare\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    import pickle\n",
    "    res = []\n",
    "    with open(file_in, \"rb\") as fin:\n",
    "        while True:\n",
    "            try:\n",
    "                res.append(pickle.load(fin))\n",
    "            except EOFError:\n",
    "                break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del file ...  caricamento completato\n",
      "Calcoliamo l'RDD del word_counter ...  calcolo completato\n",
      "text_length_in_word ...  7653\n",
      "Rapporto V/T:  0.20358029530902913\n",
      "Calcolo entropia ...  8.605861060321123\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Caricamento del file ... \", end=\" \")\n",
    "    data = load_file_without_punctuations_marks(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\")\n",
    "    print(\"caricamento completato\")\n",
    "    \n",
    "    # POSIAMO I DATI NELLA CACHE\n",
    "    data.persist()\n",
    "    \n",
    "    print(\"Calcoliamo l'RDD del word_counter ... \", end=\" \")\n",
    "    RDD_word_counter, vocabulary_size = word_counter(data)\n",
    "    print(\"calcolo completato\")\n",
    "    \n",
    "    print(\"text_length_in_word ... \", end=\" \")\n",
    "    text_length = text_length_in_words(RDD_word_counter)\n",
    "    print(text_length)\n",
    "    \n",
    "    print(\"Rapporto V/T: \", vocabulary_size/text_length)\n",
    "    \n",
    "    print(\"Calcolo entropia ... \", end=\" \")\n",
    "    RDD_hentropy = hentropy(RDD_word_counter, text_length)\n",
    "    print(getValue(RDD_hentropy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo della distribuzione di probabilitÃ  delle 30 parole piÃ¹ comuni ... [('the', 0.04494969293087678), ('i', 0.036717627074349925), ('and', 0.033320266562132494), ('to', 0.023650855873513656), ('of', 0.021298837057363126), ('a', 0.019730824513262774), ('my', 0.01620279628903698), ('that', 0.016072128577028617), ('in', 0.015941460865020254), ('was', 0.01528812230497844), ('said', 0.012152097216777734), ('as', 0.011106755520710831), ('he', 0.011106755520710831), ('at', 0.009930746112635568), ('but', 0.009669410688618842), ('you', 0.009277407552593753), ('for', 0.009146739840585392), ('me', 0.00862406899255194), ('it', 0.00862406899255194), ('had', 0.007840062720501764), ('with', 0.00718672416045995), ('not', 0.006533385600418136), (\"o'conor\", 0.006272050176401411), ('all', 0.006141382464393048), ('his', 0.005880047040376323), ('were', 0.00574937932836796), ('on', 0.005618711616359597), ('so', 0.004965373056317784), ('there', 0.004704037632301058), ('we', 0.004704037632301058)]\n",
      "\n",
      "Calcolo della probabilitÃ  di the ... ('the', 0.04494969293087678)\n",
      "Calcolo della probabilitÃ  della parola piÃ¹ comune escluso the e and ... ('i', 0.036717627074349925)\n"
     ]
    }
   ],
   "source": [
    "    print(\"Calcolo della distribuzione di probabilitÃ  delle 30 parole piÃ¹ comuni ...\", end=\" \")\n",
    "    RDD_prob_distr_of_30 = prob_distr_of_30_most_common_words(RDD_word_counter, text_length)\n",
    "    print(getCollection(RDD_prob_distr_of_30), end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"Calcolo della probabilitÃ  di the ...\", end=\" \")\n",
    "    RDD_prob_the = prob_of_The(RDD_word_counter, text_length)\n",
    "    print(getValue(RDD_prob_the))\n",
    "    \n",
    "    print(\"Calcolo della probabilitÃ  della parola piÃ¹ comune escluso the e and ...\", end=\" \")\n",
    "    RDD_prob_the_most_common_word = prob_of_the_most_common_word(RDD_word_counter, text_length)\n",
    "    print(getValue(RDD_prob_the_most_common_word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo della distanza media fra consecutive MCW ... 27.15357142857143\n",
      "Calcolo della distanza minima fra consecutive MCW ... 2\n",
      "Calcolo della distanza massima fra consecutive MCW ... 177\n",
      "Calcolo della distanza media fra consecutive the ... 22.244897959183675\n",
      "Calcolo della distanza minima fra consecutive the ... 2\n",
      "Calcolo della distanza massima fra consecutive the ... 115\n"
     ]
    }
   ],
   "source": [
    "    print(\"Calcolo della distanza media fra consecutive MCW ...\", end=\" \")\n",
    "    avg_dist_consec_MCW = avg_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    print((avg_dist_consec_MCW))\n",
    "    \n",
    "    print(\"Calcolo della distanza minima fra consecutive MCW ...\", end=\" \")\n",
    "    min_dist_consec_MCW = min_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    print((min_dist_consec_MCW))\n",
    "    \n",
    "    print(\"Calcolo della distanza massima fra consecutive MCW ...\", end=\" \")\n",
    "    max_dist_consec_MCW = max_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    print((max_dist_consec_MCW))\n",
    "    \n",
    "    print(\"Calcolo della distanza media fra consecutive the ...\", end=\" \")\n",
    "    avg_dist_consec_the = avg_distance_consec_appear(data, \"the\")\n",
    "    print((avg_dist_consec_the))\n",
    "    \n",
    "    print(\"Calcolo della distanza minima fra consecutive the ...\", end=\" \")\n",
    "    min_dist_consec_the = min_distance_consec_appear(data, \"the\")\n",
    "    print((min_dist_consec_the))\n",
    "    \n",
    "    print(\"Calcolo della distanza massima fra consecutive the ...\", end=\" \")\n",
    "    max_dist_consec_the = max_distance_consec_appear(data, \"the\")\n",
    "    print((max_dist_consec_the))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCWx = i\n",
      "Calcolo della distanza media fra consecutive MCWx ... 27.15357142857143\n",
      "Calcolo della distanza minima fra consecutive MCWx ... 2\n",
      "Calcolo della distanza massima fra consecutive MCWx ... 177\n"
     ]
    }
   ],
   "source": [
    "    MCWx = getValue(prob_of_the_most_common_word_x(RDD_word_counter, text_length))[0]\n",
    "    \n",
    "    print(\"MCWx =\", MCWx)\n",
    "    \n",
    "    print(\"Calcolo della distanza media fra consecutive MCWx ...\", end=\" \")\n",
    "    avg_dist_consec_MCWx = avg_distance_consec_appear(data, MCWx)\n",
    "    print((avg_dist_consec_MCWx))\n",
    "    \n",
    "    print(\"Calcolo della distanza minima fra consecutive MCWx ...\", end=\" \")\n",
    "    min_dist_consec_MCWx = min_distance_consec_appear(data, MCWx)\n",
    "    print((min_dist_consec_MCWx))\n",
    "    \n",
    "    print(\"Calcolo della distanza massima fra consecutive MCWx ...\", end=\" \")\n",
    "    max_dist_consec_MCWx = max_distance_consec_appear(data, MCWx)\n",
    "    print((max_dist_consec_MCWx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSL:\t 84\n",
      "ASL:\t 14.862135922330097\n",
      "mSL:\t 1\n",
      "PMSL:\t [(7, 0.06407766990291262)]\n",
      "PdSL:\t [(7, 0.06407766990291262), (2, 0.05631067961165048), (9, 0.04660194174757282), (10, 0.04660194174757282), (17, 0.04271844660194175), (12, 0.040776699029126215), (8, 0.038834951456310676), (16, 0.038834951456310676), (1, 0.038834951456310676), (15, 0.038834951456310676), (3, 0.038834951456310676), (6, 0.036893203883495145), (5, 0.03495145631067961), (11, 0.03495145631067961), (13, 0.03300970873786408), (4, 0.031067961165048542), (20, 0.02912621359223301), (25, 0.02330097087378641), (22, 0.02330097087378641), (18, 0.021359223300970873), (14, 0.021359223300970873), (29, 0.019417475728155338), (23, 0.019417475728155338), (19, 0.019417475728155338), (21, 0.017475728155339806), (24, 0.015533980582524271), (28, 0.013592233009708738), (26, 0.013592233009708738), (35, 0.011650485436893204), (30, 0.009708737864077669), (31, 0.009708737864077669), (27, 0.007766990291262136), (41, 0.005825242718446602), (37, 0.005825242718446602), (33, 0.005825242718446602), (38, 0.005825242718446602), (59, 0.005825242718446602), (32, 0.003883495145631068), (46, 0.003883495145631068), (34, 0.003883495145631068), (48, 0.001941747572815534), (84, 0.001941747572815534), (56, 0.001941747572815534), (40, 0.001941747572815534), (61, 0.001941747572815534), (54, 0.001941747572815534), (50, 0.001941747572815534), (42, 0.001941747572815534), (39, 0.001941747572815534), (63, 0.001941747572815534), (43, 0.001941747572815534)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    sentences_data = load_file_without_number(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\")\n",
    "    sentences_data.persist()\n",
    "    \n",
    "    sen_lengths = sentence_lengths(sentences_data)\n",
    "    print(\"MSL:\\t\", max(lengths))\n",
    "    print(\"ASL:\\t\", sum(lengths)/len(lengths))\n",
    "    print(\"mSL:\\t\", min(lengths))\n",
    "    \n",
    "    RDD_sen_lengths = sc.parallelize(sen_lengths)\n",
    "    \n",
    "    prob_most_freq_sen = prob_of_the_most_freq_sentence_len(RDD_sen_lengths)\n",
    "    print(\"PMSL:\\t\", prob_most_freq_sen)\n",
    "    \n",
    "    prob_distr_freq_sen = prob_distr_of_sentence_length(RDD_sen_lengths)\n",
    "    print(\"PdSL:\\t\", getCollection(prob_distr_freq_sen), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-9044aaa569a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp_comma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob_of_comma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pcomma:\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_comma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_dist_consec_appear_comma:\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_distance_consec_appear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min_dist_consec_appear_comma:\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_distance_consec_appear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences_data' is not defined"
     ]
    }
   ],
   "source": [
    "    p_comma = prob_of_comma(sentences_data, text_length)\n",
    "    print(\"Pcomma:\\t\", p_comma)\n",
    "    \n",
    "    print(\"max_dist_consec_appear_comma:\\t\", max_distance_consec_appear(sentences_data, \",\"))\n",
    "    print(\"min_dist_consec_appear_comma:\\t\", min_distance_consec_appear(sentences_data, \",\"))\n",
    "    print(\"avg_dist_consec_appear_comma:\\t\", avg_distance_consec_appear(sentences_data, \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del file ...  caricamento completato\n",
      "Calcolo le metriche attendere ...  calcolo completato\n",
      "Salvataggio completato\n",
      "Caricamento del file ...  caricamento completato\n",
      "Calcolo le metriche attendere ...  calcolo completato\n",
      "Salvataggio completato\n"
     ]
    }
   ],
   "source": [
    "    save_metrics(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\", \"Anthony Trollope\")\n",
    "    save_metrics(\"datasets/Anthony Trollope___George Walker At Suez.txt\", \"Anthony Trollope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocabulary_size': (1586, 39.59797974644666), 'text_length': (7877, 316.7838379715733), 'V_T': (0.20140747884819438, 0.0030728265074600365), 'hentropy': (8.519433709418967, 0.1222267318058073), 'prob_distr_of_30': [('i', (0.039899364086489864, 0.004499655634472745)), ('of', (0.02385766442425001, 0.003618728366022868)), ('in', (0.017722612916154123, 0.0025189293873621715)), ('was', (0.01616152813187448, 0.0012351823658520702)), ('he', (0.011169968304732653, 8.939637647902112e-05)), ('but', (0.009155159609214989, 0.0007272608509579331)), ('there', (0.002352018816150529, 0.0033262569087567895)), ('have', (0.0031477595358597706, 0.004451604226702126)), ('would', (0.002468831008517467, 0.0034914542954526478)), ('the', (0.05117500693945394, 0.008803923500961045)), ('and', (0.03369506723983677, 0.0005300482015960147)), ('to', (0.025836043910093455, 0.0030903225576665847)), ('a', (0.02060482714368237, 0.0012360263734891516)), ('that', (0.018837199950778226, 0.003910401437686302)), ('you', (0.004638703776296877, 0.006560117792270335)), ('be', (0.0029008764350080237, 0.004102458797156861)), ('which', (0.002839155659795087, 0.004015172439770545)), ('said', (0.006076048608388867, 0.008592830347621707)), ('as', (0.012095779932926702, 0.001398691737273764)), ('at', (0.011260892128037325, 0.0018811105349176966)), ('his', (0.006766711583390235, 0.001253933022005617)), ('we', (0.002352018816150529, 0.0033262569087567895)), ('this', (0.0025922725589433405, 0.0036660270102252803)), ('my', (0.013903151014534538, 0.0032521895358485534)), ('for', (0.007412525580087783, 0.0024525493272565466)), ('me', (0.00986690426544027, 0.0017576344987143434)), ('it', (0.009805183490227333, 0.001670348141328027)), ('had', (0.011264803610590346, 0.004843315014376977)), ('with', (0.007852095569922606, 0.0009409772712773896)), ('not', (0.007525426289901699, 0.0014029573974936105)), (\"o'conor\", (0.0031360250882007056, 0.00443500921167572)), ('all', (0.005477801465501054, 0.0009384452483661434)), ('were', (0.00287468966418398, 0.004065425110702743)), ('on', (0.007253251623511239, 0.0023115886463552673)), ('so', (0.002482686528158892, 0.003511048959243278)), ('been', (0.0031477595358597706, 0.004451604226702126)), ('could', (0.00222194790766572, 0.003142308865907383))], 'prob_of_the_most_common_word': (0.039899364086489864, 0.004499655634472745), 'prob_of_the': (0.05117500693945394, 0.008803923500961045), 'prob_of_comma': (0.067912805327707, 0.0003209282012460747), 'avg_dist_consec_MCW': (25.164429392446635, 2.813071644974118), 'min_dist_consec_MCW': (1.5, 0.7071067811865476), 'max_dist_consec_MCW': (246, 97.58073580374356), 'avg_dist_consec_the': (19.816414496833218, 3.43439424845479), 'min_dist_consec_the': (2, 0.0), 'max_dist_consec_the': (113, 2.8284271247461903), 'avg_dist_consec_MCWx': (25.164429392446635, 2.813071644974118), 'min_dist_consec_MCWx': (1.5, 0.7071067811865476), 'max_dist_consec_MCWx': (246, 97.58073580374356), 'avg_dist_consec_comma': (14.68808250867949, 0.0362753793884873), 'min_dist_consec_comma': (1, 0.0), 'max_dist_consec_comma': (88.5, 4.949747468305833), 'avg_sentence_len': (18.620294480502064, 5.314838802515312), 'max_sentence_len': (77, 9.899494936611665), 'min_sentence_len': (1, 0.0), 'prob_most_freq_sen': (0.056900713404494985, 0.010149749216623944), 'prob_distr_freq_sen': [(12, (0.034200504210695704, 0.009300143901032102)), (8, (0.030467199485061415, 0.011833788324314906)), (16, (0.03599206136351445, 0.004020453725568525)), (4, (0.02105884246097731, 0.014155031418699754)), (20, (0.03804376977954192, 0.012611328903316366)), (24, (0.01881671404816821, 0.004642486188769695)), (28, (0.017845840261760445, 0.006015509064860078)), (32, (0.008847824920881832, 0.007020622496252209)), (48, (0.007876951134474066, 0.008393645372342592)), (84, (0.000970873786407767, 0.0013730228760903834)), (56, (0.000970873786407767, 0.0013730228760903834)), (40, (0.0023520892560210263, 0.0005803107735962118)), (36, (0.0013812154696132596, 0.001953333649686595)), (44, (0.0013812154696132596, 0.001953333649686595)), (64, (0.0013812154696132596, 0.001953333649686595)), (9, (0.03849434103953227, 0.011465878879616658)), (17, (0.04207745534516977, 0.0009064985286895079)), (1, (0.023561122136995117, 0.021600456572747884)), (5, (0.021619374564179585, 0.01885441082056712)), (13, (0.031698224534677896, 0.0018547187469839745)), (25, (0.018556562784959502, 0.006709606264651627)), (29, (0.022139677090597006, 0.0038497740862755234)), (21, (0.029456096121868797, 0.016942798860485477)), (41, (0.002912621359223301, 0.00411906862827115)), (37, (0.00843748323767634, 0.0036942659704752305)), (33, (0.013962345116129378, 0.01150760056922161)), (61, (0.000970873786407767, 0.0013730228760903834)), (53, (0.0027624309392265192, 0.00390666729937319)), (45, (0.0027624309392265192, 0.00390666729937319)), (49, (0.0013812154696132596, 0.001953333649686595)), (2, (0.035061417153891536, 0.030050995158188144)), (10, (0.04125677197875878, 0.007559211580243467)), (6, (0.02120903288097409, 0.022180767346344096)), (22, (0.03651236388993188, 0.018683731181274112)), (18, (0.02725419728584455, 0.008336752159244924)), (14, (0.021729335407391512, 0.0005234175604985436)), (26, (0.023370702140213483, 0.013828843663606458)), (30, (0.017285308158558173, 0.01071488846672744)), (38, (0.012581129646516118, 0.009554266919535016)), (46, (0.0033229630424287935, 0.0007927121024941717)), (34, (0.01437268679933487, 0.014833957094998589)), (54, (0.0051145201952475456, 0.004486978072969402)), (50, (0.000970873786407767, 0.0013730228760903834)), (42, (0.003733304725634286, 0.002533644423282807)), (70, (0.0013812154696132596, 0.001953333649686595)), (62, (0.0013812154696132596, 0.001953333649686595)), (66, (0.0013812154696132596, 0.001953333649686595)), (7, (0.051375851526041946, 0.017963083815370325)), (15, (0.033229630424287934, 0.007927121024941716)), (3, (0.020798691197768598, 0.025507123872121073)), (11, (0.027144236442632624, 0.011041076221820737)), (23, (0.026283323499436784, 0.00970977503533531)), (19, (0.026283323499436784, 0.00970977503533531)), (35, (0.01411253553612616, 0.0034818646415772704)), (31, (0.013141661749718392, 0.004854887517667655)), (27, (0.016314434372150403, 0.012087911342817823)), (59, (0.00567505229844982, 0.00021240132889796016)), (39, (0.006495735664860805, 0.006440311722655997)), (63, (0.000970873786407767, 0.0013730228760903834)), (43, (0.003733304725634286, 0.002533644423282807)), (47, (0.0027624309392265192, 0.00390666729937319)), (51, (0.0013812154696132596, 0.001953333649686595))]}\n"
     ]
    }
   ],
   "source": [
    "    import statistics\n",
    "    \n",
    "    def mean_std_couple(_list, tot_el):\n",
    "        for i in range(0, tot_el - len(_list)):\n",
    "            _list.append(0)\n",
    "\n",
    "        return (statistics.mean(_list), statistics.stdev(_list))\n",
    "    \n",
    "    \n",
    "    analisi = load_metrics(\"Anthony Trollope\")\n",
    "    author_metrics = dict()\n",
    "    \n",
    "    for diz in analisi:\n",
    "        for key in diz:\n",
    "            try:\n",
    "                author_metrics[key] += [diz[key]]\n",
    "            except:\n",
    "                author_metrics[key] = [diz[key]]\n",
    "                \n",
    "\n",
    "    for key in author_metrics:\n",
    "        if type(author_metrics[key][0]) != list:\n",
    "            author_metrics[key] = (statistics.mean(author_metrics[key]), statistics.stdev(author_metrics[key]))\n",
    "        else:\n",
    "            tot_el = len(author_metrics[key])\n",
    "            author_metrics[key] = (sc.parallelize(author_metrics[key])\n",
    "                        .flatMap(lambda x: x)\n",
    "                        .map(lambda x: (x[0], [x[1]]))\n",
    "                        .reduceByKey(lambda a,b: a+b).map(lambda x: (x[0], mean_std_couple(x[1], tot_el)))\n",
    "                       ).collect()\n",
    "    print(author_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del file ...  caricamento completato\n",
      "Calcolo le metriche attendere ...  calcolo completato\n"
     ]
    }
   ],
   "source": [
    "    # genero le metriche dello stesso autore\n",
    "    \n",
    "    #test_metrics = generate_metrics(\"datasets/Anthony Trollope___The Relics of General Chasse.txt\") \n",
    "    \n",
    "    test_metrics = generate_metrics(\"datasets/Andrew Lang___How to Fail in Literature.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.34111533843908 %\n"
     ]
    }
   ],
   "source": [
    "    # supponendo una distribuzione normalizzata \n",
    "    # calcolo un valore compreso fra 0 e 1 secondo la funzione di distribuzione di probabilitÃ \n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    def gaussian(x, mu, sigma):\n",
    "        if sigma != 0:\n",
    "            return np.exp(-np.power((x - mu)/sigma, 2)/2)\n",
    "        return 1 if x == mu else 0\n",
    "    \n",
    "    def search_tuple(_list, value):\n",
    "        for tup in _list:\n",
    "            if tup[0] == value:\n",
    "                return tup\n",
    "        return None\n",
    "        \n",
    "    score = 0\n",
    "    total = 0\n",
    "    \n",
    "    for key in test_metrics:\n",
    "        if type(test_metrics[key]) != list:\n",
    "            score += gaussian(test_metrics[key], author_metrics[key][0], author_metrics[key][1])\n",
    "            total += 1\n",
    "        else:\n",
    "            pass\n",
    "            for tup in test_metrics[key]:\n",
    "                res_search = search_tuple(author_metrics[key], tup[0])\n",
    "                \n",
    "                if res_search != None:\n",
    "                    score += gaussian(tup[1], res_search[1][0], res_search[1][1])\n",
    "                    total += 1\n",
    "                \n",
    "                else:\n",
    "                    # penalita' ?????\n",
    "                    total += 1\n",
    "    \n",
    "    print(score/total * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
