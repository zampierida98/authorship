{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi sull'intero testo\n",
    "\n",
    "- Dimensione del vocabolario (V)\n",
    "- Lunghezza del testo in numero di parole (T)\n",
    "- Rapporto V/T\n",
    "- Entropia (H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def word_counter(RDD):\n",
    "    '''\n",
    "    Data una RDD, conta quante volte compare ogni parola ritornando anche la dimensione del vocabolario.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD : RDD\n",
    "        RDD del file in input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (RDD, int)\n",
    "        RDD dell'output del word count e dimensione del vocabolario\n",
    "    '''\n",
    "    \n",
    "    word_counter = (RDD.flatMap(lambda x: x)\n",
    "                    .map(lambda x: (x,1))\n",
    "                    .reduceByKey(lambda a,b: a+b)\n",
    "                    .sortBy(lambda x: -x[1])\n",
    "                   )\n",
    "    \n",
    "    return word_counter, word_counter.count()\n",
    "\n",
    "def text_length_in_words(RDD_word_counter):\n",
    "    '''\n",
    "    Calcola la lunghezza del testo in termini di numero di parole.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_word_counter : RDD\n",
    "        RDD dell'output del word count\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        numero di parole totali presenti nel testo\n",
    "    '''\n",
    "\n",
    "    return (RDD_word_counter.map(lambda x: x[1])\n",
    "            .reduce(lambda a,b: a+b)\n",
    "           )\n",
    "\n",
    "def entropy(RDD_word_counter, text_len):\n",
    "    '''\n",
    "    Calcola l'entropia (numero medio di bit richiesti per rappresentare tutte le parole del testo).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_word_counter : RDD\n",
    "        RDD dell'output del word count\n",
    "    text_len : int\n",
    "        numero di parole totali presenti nel testo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        valore dell'entropia\n",
    "    '''\n",
    "    \n",
    "    return -(RDD_word_counter.map(lambda x: (x[1]/text_len) * math.log2(x[1]/text_len))\n",
    "             .reduce(lambda a,b: a+b)\n",
    "            ) # l'entropia ha segno negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi sulle frasi\n",
    "\n",
    "- Lunghezza massima di una frase (MSL)\n",
    "- Lunghezza media di una frase (ASL)\n",
    "- Lunghezza minima di una frase (mSL)\n",
    "- Distribuzione di probabilità delle lunghezze delle frasi (PDSL)\n",
    "- Probabilità della lunghezza di frase più frequente (pMFSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_lengths(RDD):\n",
    "    '''\n",
    "    Calcola le lunghezze (in termini di numero di parole) di tutte le frasi di un testo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD : RDD\n",
    "        RDD del file in input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RDD\n",
    "        RDD che contiene le lunghezze delle frasi\n",
    "    '''\n",
    "    \n",
    "    # operazioni preliminari sul testo\n",
    "    text = RDD.flatMap(lambda x: x).reduce(lambda a,b: a + ' ' + b) # metto tutto il testo in una stringa unica\n",
    "    text = text.replace(\"?\", \".\") # ? termina una frase\n",
    "    text = text.replace(\"!\", \".\") # ! termina una frase\n",
    "    text = text.split('. ') # splitto quando trovo un carattere che termina una frase (. seguito da uno spazio)\n",
    "    \n",
    "    return (sc.parallelize(text)\n",
    "            .map(lambda x: len(x.split(' ')))\n",
    "           ) # per ogni frase trovata conto le sue parole\n",
    "\n",
    "def prob_distr_of_sentence_length(RDD_sen_len):\n",
    "    '''\n",
    "    Ritorna una lista che contiene la distribuzione di probabilità delle lunghezze delle frasi.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_sen_len : RDD\n",
    "        RDD dell'output di sentence_lengths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        distribuzione di probabilità delle lunghezze delle frasi\n",
    "    '''\n",
    "    \n",
    "    tot = RDD_sen_len.count()\n",
    "\n",
    "    return (RDD_sen_len.map(lambda x: (x,1))\n",
    "            .reduceByKey(lambda a,b: a+b)\n",
    "            .map(lambda x: (x[0], x[1]/tot))\n",
    "            .sortBy(lambda x: -x[1])\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi sulla probabilità delle parole\n",
    "\n",
    "- Distribuzione di probabilità delle 30 parole più comuni (PkMCW)\n",
    "- Probabilità della parola più comune escludendo 'and' e 'the' (pMCW)\n",
    "- Probabilità della parola più comune escludendo articoli e preposizioni (pMCWx)\n",
    "- Probabilità della parola 'the' (pThe)\n",
    "- Probabilità della virgola (pComma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_distr_of_most_common_words(RDD_word_counter, text_len):\n",
    "    '''\n",
    "    Ritorna la distribuzione di probabilità delle parole più comuni.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_word_counter : RDD\n",
    "        RDD dell'output del word count\n",
    "    text_len : int\n",
    "        numero di parole totali presenti nel testo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RDD\n",
    "        RDD che contiene la distribuzione di probabilità\n",
    "    '''\n",
    "    \n",
    "    return RDD_word_counter.map(lambda x: (x[0], x[1]/text_len))\n",
    "\n",
    "def prob_of_the_most_common_word(RDD_prob_distr_of_MCWs):\n",
    "    '''\n",
    "    Ritorna la probabilità della parola più comune (escludendo 'and' e 'the').\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_prob_distr_of_MCWs : RDD\n",
    "        RDD con la distribuzione di probabilità\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        MCW e relativa probabilità\n",
    "    '''\n",
    "    \n",
    "    return (RDD_prob_distr_of_MCWs\n",
    "            .filter(lambda x: x[0] != \"and\" and x[0] != \"the\")\n",
    "            .take(1)\n",
    "           )[0]\n",
    "\n",
    "def prob_of_the_most_common_word_x(RDD_prob_distr_of_MCWs):\n",
    "    '''\n",
    "    Ritorna la probabilità della parola più comune (escludendo articoli e preposizioni).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_prob_distr_of_MCWs : RDD\n",
    "        RDD con la distribuzione di probabilità\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        MCWx e relativa probabilità\n",
    "    '''\n",
    "    \n",
    "    prep_art = open(\"preposizioni_e_articoli.txt\").read().splitlines()\n",
    "    \n",
    "    return (RDD_prob_distr_of_MCWs\n",
    "            .filter(lambda x: x[0] not in prep_art)\n",
    "            .take(1)\n",
    "           )[0]\n",
    "\n",
    "def prob_of_The(RDD_prob_distr_of_MCWs):\n",
    "    '''\n",
    "    Ritorna la probabilità della parola \"the\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_prob_distr_of_MCWs : RDD\n",
    "        RDD con la distribuzione di probabilità\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    RDD\n",
    "        RDD che contiene la probabilità della parola \"the\"\n",
    "    '''\n",
    "    \n",
    "    return (RDD_prob_distr_of_MCWs\n",
    "            .filter(lambda x: x[0] == \"the\")\n",
    "           )\n",
    "\n",
    "def prob_of_comma(RDD_sentences_data, text_len):\n",
    "    '''\n",
    "    Ritorna la probabilità di presenza della virgola.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD_sentences_data : RDD\n",
    "        RDD del file in input\n",
    "    text_len : int\n",
    "        numero di parole totali presenti nel testo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        probabilità di presenza della virgola\n",
    "    '''\n",
    "    \n",
    "    return (RDD_sentences_data\n",
    "            .flatMap(lambda x: x)\n",
    "            .filter(lambda x: \",\" in x)\n",
    "            .count()\n",
    "           ) / text_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributi sulla distanza\n",
    "\n",
    "- Distanza media/minima/massima tra apparenze consecutive della MCW (adMCW, mdMCW, MsMCW)\n",
    "- Distanza media/minima/massima tra apparenze consecutive della MCW escludendo articoli e preposizioni (adMCWx, mdMCWx, MsMCWx)\n",
    "- Distanza media/minima/massima tra apparenze consecutive di 'the' (adThe, mdThe, MsThe)\n",
    "- Distanza media/minima/massima tra apparenze consecutive della virgola (adComma, mdComma, MsComma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_consec_appear(RDD, word):\n",
    "    '''\n",
    "    Ritorna una lista che contiene le distanze tra apparenze consecutive di word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RDD : RDD\n",
    "        RDD del file in input\n",
    "    word : str\n",
    "        parola da trattare\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        distanze tra apparenze consecutive di word\n",
    "    '''\n",
    "    \n",
    "    if word == ',':\n",
    "        vect_pos = (RDD.flatMap(lambda x:x)\n",
    "                    .zipWithIndex()\n",
    "                    .filter(lambda x: ',' in x[0])\n",
    "                    .map(lambda x: x[1])\n",
    "                    .collect()\n",
    "                   )\n",
    "    else:\n",
    "        vect_pos = (RDD.flatMap(lambda x:x)\n",
    "                    .zipWithIndex()\n",
    "                    .filter(lambda x: x[0] == word)\n",
    "                    .map(lambda x: x[1])\n",
    "                    .collect()\n",
    "                   )\n",
    "    \n",
    "    vect_dis = []\n",
    "    \n",
    "    for i in range(1, len(vect_pos)):\n",
    "        vect_dis.append(vect_pos[i] - vect_pos[i-1])\n",
    "    \n",
    "    return vect_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio degli attributi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import statistics\n",
    "\n",
    "def generate_metrics(file_in):\n",
    "    '''\n",
    "    Ritorna un dizionario contenente tutti gli attributi estratti da un testo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_in : str\n",
    "        path del file da analizzare\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dizionario degli attributi estratti\n",
    "    '''\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    # consideriamo il testo SENZA i segni di punteggiatura\n",
    "    print(\"Caricamento del file in memoria ...\", end=\" \")\n",
    "    data = load_file_without_punctuations_marks(file_in)\n",
    "    data.persist()\n",
    "    print(\"caricamento completato\")\n",
    "\n",
    "    # calcoliamo le prime metriche\n",
    "    print(\"Calcolo delle prime metriche, attendere ...\", end=\" \")\n",
    "    \n",
    "    RDD_word_counter, vocabulary_size = word_counter(data)\n",
    "    text_length = text_length_in_words(RDD_word_counter)\n",
    "    entropy_value = entropy(RDD_word_counter, text_length)\n",
    "    \n",
    "    #***DA RIVEDERE***\n",
    "    '''\n",
    "    RDD_prob_distr_of_30 = prob_distr_of_30_most_common_words(RDD_word_counter, text_length)\n",
    "    RDD_prob_the_most_common_word = prob_of_the_most_common_word(RDD_word_counter, text_length)\n",
    "    RDD_prob_the = prob_of_The(RDD_word_counter, text_length)\n",
    "\n",
    "    avg_dist_consec_MCW = avg_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    min_dist_consec_MCW = min_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "    max_dist_consec_MCW = max_distance_consec_appear(data, getValue(RDD_prob_the_most_common_word)[0])\n",
    "\n",
    "    avg_dist_consec_the = avg_distance_consec_appear(data, \"the\")\n",
    "    min_dist_consec_the = min_distance_consec_appear(data, \"the\")\n",
    "    max_dist_consec_the = max_distance_consec_appear(data, \"the\")\n",
    "    \n",
    "    # genero MCWx da passare come parametro alle funzioni che calcolano le distanze\n",
    "    MCWx = getValue(prob_of_the_most_common_word_x(RDD_word_counter, text_length))[0]\n",
    "    \n",
    "    avg_dist_consec_MCWx = avg_distance_consec_appear(data, MCWx)\n",
    "    min_dist_consec_MCWx = min_distance_consec_appear(data, MCWx)\n",
    "    max_dist_consec_MCWx = max_distance_consec_appear(data, MCWx)\n",
    "    '''\n",
    "    #***DA RIVEDERE***\n",
    "    \n",
    "    print(\"calcolo completato\")\n",
    "\n",
    "    \n",
    "    # consideriamo il testo CON i segni di punteggiatura\n",
    "    print(\"Caricamento del file in memoria ...\", end=\" \")\n",
    "    sentences_data = load_file_without_number(file_in)\n",
    "    sentences_data.persist()\n",
    "    print(\"caricamento completato\")\n",
    "    \n",
    "    # calcoliamo altre metriche\n",
    "    print(\"Calcolo di ulteriori metriche, attendere ...\", end=\" \")\n",
    "    \n",
    "    RDD_sen_lengths = sentence_lengths(sentences_data)\n",
    "    RDD_sen_lengths.persist()\n",
    "\n",
    "    sen_lengths = RDD_sen_lengths.collect()\n",
    "\n",
    "    prob_distr_freq_sen = prob_distr_of_sentence_length(RDD_sen_lengths)\n",
    "    \n",
    "    p_comma = prob_of_comma(sentences_data, text_length)\n",
    "\n",
    "    dist_consec_comma = distance_consec_appear(sentences_data, ',')\n",
    "    \n",
    "    print(\"calcolo completato\")\n",
    "    \n",
    "    \n",
    "    # popoliamo il dizionario\n",
    "    \n",
    "    # attributi sull'intero testo\n",
    "    res['vocabulary_size'] = vocabulary_size\n",
    "    res['text_length'] = text_length\n",
    "    res['V_T'] = vocabulary_size/text_length\n",
    "    res['entropy'] = entropy_value\n",
    "    \n",
    "    # attributi sulle frasi\n",
    "    res['avg_sentence_len'] = sum(sen_lengths)/len(sen_lengths)\n",
    "    res['max_sentence_len'] = max(sen_lengths)\n",
    "    res['min_sentence_len'] = min(sen_lengths)\n",
    "    res['prob_distr_freq_sen'] = getCollection(prob_distr_freq_sen)\n",
    "    res['prob_most_freq_sen'] = getValue(prob_distr_freq_sen)\n",
    "    \n",
    "    # attributi sulla probabilità delle parole\n",
    "    \n",
    "    # attributi sulla distanza\n",
    "    res['avg_dist_consec_comma'] = sum(dist_consec_comma)/len(dist_consec_comma)\n",
    "    res['min_dist_consec_comma'] = min(dist_consec_comma)\n",
    "    res['max_dist_consec_comma'] = max(dist_consec_comma)\n",
    "    \n",
    "    '''\n",
    "    res['prob_distr_of_30'] = getCollection(RDD_prob_distr_of_30)\n",
    "    res['prob_of_the_most_common_word'] = getValue(RDD_prob_the_most_common_word)[1]\n",
    "    res['prob_of_the'] = getValue(RDD_prob_the)[1]\n",
    "    res['prob_of_comma'] = p_comma\n",
    "    \n",
    "    res['avg_dist_consec_MCW'] = avg_dist_consec_MCW\n",
    "    res['min_dist_consec_MCW'] = min_dist_consec_MCW\n",
    "    res['max_dist_consec_MCW'] = max_dist_consec_MCW\n",
    "    \n",
    "    res['avg_dist_consec_the'] = avg_dist_consec_the\n",
    "    res['min_dist_consec_the'] = min_dist_consec_the\n",
    "    res['max_dist_consec_the'] = max_dist_consec_the\n",
    "    \n",
    "    res['avg_dist_consec_MCWx'] = avg_dist_consec_MCWx\n",
    "    res['min_dist_consec_MCWx'] = min_dist_consec_MCWx\n",
    "    res['max_dist_consec_MCWx'] = max_dist_consec_MCWx\n",
    "    '''\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def save_metrics(file_in, file_out):\n",
    "    '''\n",
    "    Salva un dizionario in un file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_in : str\n",
    "        path del file da analizzare\n",
    "    file_out: str\n",
    "        path del file in cui salvare la entry\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "\n",
    "    entry = generate_metrics(file_in)\n",
    "    \n",
    "    with open(file_out, 'ab') as fout:\n",
    "        pickle.dump(entry, fout, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Salvataggio completato\")\n",
    "\n",
    "def load_metrics(file_in):\n",
    "    '''\n",
    "    Carica i dizionari degli attributi presenti in un file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_in : str\n",
    "        path del file da cui caricare\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        lista di dizionari degli attributi\n",
    "    '''\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    with open(file_in, \"rb\") as fin:\n",
    "        while True:\n",
    "            try:\n",
    "                res.append(pickle.load(fin))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "    return res\n",
    "\n",
    "def mean_std_couple(_list, tot_el):\n",
    "        for i in range(0, tot_el - len(_list)):\n",
    "            _list.append(0)\n",
    "\n",
    "        return (statistics.mean(_list), statistics.stdev(_list))\n",
    "    \n",
    "def author_metrics(author_name):\n",
    "    analisi = load_metrics(author_name)\n",
    "    res = dict()\n",
    "    \n",
    "    for diz in analisi:\n",
    "        for key in diz:\n",
    "            try:\n",
    "                res[key] += [diz[key]]\n",
    "            except:\n",
    "                res[key] = [diz[key]]\n",
    "                \n",
    "    for key in res:    \n",
    "        if type(res[key][0]) != list:\n",
    "            res[key] = (statistics.mean(res[key]), statistics.stdev(res[key]))\n",
    "        else:\n",
    "            tot_el = len(res[key])\n",
    "            res[key] = (sc.parallelize(res[key])\n",
    "                        .flatMap(lambda x: x)\n",
    "                        .map(lambda x: (x[0], [x[1]]))\n",
    "                        .reduceByKey(lambda a,b: a+b).map(lambda x: (x[0], mean_std_couple(x[1], tot_el)))\n",
    "                       ).collect()\n",
    "    \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[155] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_number_some_punctuation_marks(row):\n",
    "    lowercase = row.lower()\n",
    "    lowercase = lowercase.replace(\"--\", \" \")\n",
    "    \n",
    "    res = \"\"\n",
    "    \n",
    "    for char in lowercase:\n",
    "        if not ('0' <= char <= '9' or char == '\"'):\n",
    "            res += char\n",
    "\n",
    "    return res\n",
    "\n",
    "def remove_number_punctuation_marks(row):\n",
    "    lowercase = row.lower()\n",
    "    lowercase = lowercase.replace(\"--\", \" \")\n",
    "    \n",
    "    res = \"\"\n",
    "    \n",
    "    for char in lowercase:\n",
    "        if 'a' <= char <= 'z' or char == ' ' or char == '-' or char == \"'\":\n",
    "            res += char\n",
    "\n",
    "    return res\n",
    "\n",
    "def load_file_without_punctuations_marks(filepath):\n",
    "    # caricamento del dataset\n",
    "    raw_text = sc.textFile(filepath)\n",
    "\n",
    "    # rimuoviamo i numeri e i segni di punteggiatura\n",
    "    \n",
    "    return (raw_text.filter(bool)                    # rimuoviamo le stringhe vuote\n",
    "        .map(remove_number_punctuation_marks)\n",
    "        .map(lambda x : ' '.join(x.split()))        # rimuoviamo diversi spazi bianchi con uno\n",
    "        .map(lambda row : row.split(\" \"))\n",
    "       )\n",
    "\n",
    "def load_file_without_number(filepath):\n",
    "    # caricamento del dataset\n",
    "    raw_text = sc.textFile(filepath)\n",
    "\n",
    "    # rimuoviamo i numeri e i segni di punteggiatura\n",
    "    \n",
    "    return (raw_text.filter(bool)                    # rimuoviamo le stringhe vuote\n",
    "        .map(remove_number_some_punctuation_marks)\n",
    "        .map(lambda x : ' '.join(x.split()))        # rimuoviamo diversi spazi bianchi con uno\n",
    "        .map(lambda row : row.split(\" \"))\n",
    "       )\n",
    "\n",
    "def getCollection(RDD):\n",
    "    return RDD.collect()\n",
    "\n",
    "def getValue(RDD):\n",
    "    return RDD.collect()[0]\n",
    "\n",
    "\n",
    "data = load_file_without_punctuations_marks(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\")\n",
    "data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V:\t 1558\n",
      "T:\t 7653\n",
      "V/T:\t 0.20358029530902913\n",
      "H:\t 8.605861060321123\n"
     ]
    }
   ],
   "source": [
    "RDD_word_counter, vocabulary_size = word_counter(data)\n",
    "RDD_word_counter.persist() # ci servirà per calcolare altri parametri\n",
    "print(\"V:\\t\", vocabulary_size)\n",
    "\n",
    "text_length = text_length_in_words(RDD_word_counter)\n",
    "print(\"T:\\t\", text_length)\n",
    "\n",
    "print(\"V/T:\\t\", vocabulary_size/text_length)\n",
    "\n",
    "entropy_value = entropy(RDD_word_counter, text_length)\n",
    "print(\"H:\\t\", entropy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[172] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_data = load_file_without_number(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\")\n",
    "sentences_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSL:\t 84\n",
      "ASL:\t 14.862135922330097\n",
      "mSL:\t 1\n",
      "PDSL:\t [(7, 0.06407766990291262), (2, 0.05631067961165048), (9, 0.04660194174757282), (10, 0.04660194174757282), (17, 0.04271844660194175), (12, 0.040776699029126215), (8, 0.038834951456310676), (16, 0.038834951456310676), (1, 0.038834951456310676), (3, 0.038834951456310676), (15, 0.038834951456310676), (6, 0.036893203883495145), (11, 0.03495145631067961), (5, 0.03495145631067961), (13, 0.03300970873786408), (4, 0.031067961165048542), (20, 0.02912621359223301), (25, 0.02330097087378641), (22, 0.02330097087378641), (18, 0.021359223300970873), (14, 0.021359223300970873), (19, 0.019417475728155338), (29, 0.019417475728155338), (23, 0.019417475728155338), (21, 0.017475728155339806), (24, 0.015533980582524271), (26, 0.013592233009708738), (28, 0.013592233009708738), (35, 0.011650485436893204), (30, 0.009708737864077669), (31, 0.009708737864077669), (27, 0.007766990291262136), (41, 0.005825242718446602), (33, 0.005825242718446602), (59, 0.005825242718446602), (37, 0.005825242718446602), (38, 0.005825242718446602), (32, 0.003883495145631068), (34, 0.003883495145631068), (46, 0.003883495145631068), (48, 0.001941747572815534), (56, 0.001941747572815534), (40, 0.001941747572815534), (50, 0.001941747572815534), (42, 0.001941747572815534), (43, 0.001941747572815534), (84, 0.001941747572815534), (61, 0.001941747572815534), (54, 0.001941747572815534), (39, 0.001941747572815534), (63, 0.001941747572815534)]\n",
      "pMFSL:\t (7, 0.06407766990291262)\n"
     ]
    }
   ],
   "source": [
    "RDD_sen_lengths = sentence_lengths(sentences_data)\n",
    "RDD_sen_lengths.persist() # ci servirà per calcolare altri parametri\n",
    "\n",
    "sen_lengths = RDD_sen_lengths.collect()\n",
    "print(\"MSL:\\t\", max(sen_lengths))\n",
    "print(\"ASL:\\t\", sum(sen_lengths)/len(sen_lengths))\n",
    "print(\"mSL:\\t\", min(sen_lengths))\n",
    "\n",
    "prob_distr_freq_sen = prob_distr_of_sentence_length(RDD_sen_lengths)\n",
    "print(\"PDSL:\\t\", getCollection(prob_distr_freq_sen))\n",
    "print(\"pMFSL:\\t\", getValue(prob_distr_freq_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PkMCW:\t [('the', 0.04494969293087678), ('i', 0.036717627074349925), ('and', 0.033320266562132494), ('to', 0.023650855873513656), ('of', 0.021298837057363126), ('a', 0.019730824513262774), ('my', 0.01620279628903698), ('that', 0.016072128577028617), ('in', 0.015941460865020254), ('was', 0.01528812230497844), ('said', 0.012152097216777734), ('as', 0.011106755520710831), ('he', 0.011106755520710831), ('at', 0.009930746112635568), ('but', 0.009669410688618842), ('you', 0.009277407552593753), ('for', 0.009146739840585392), ('me', 0.00862406899255194), ('it', 0.00862406899255194), ('had', 0.007840062720501764), ('with', 0.00718672416045995), ('not', 0.006533385600418136), (\"o'conor\", 0.006272050176401411), ('all', 0.006141382464393048), ('his', 0.005880047040376323), ('were', 0.00574937932836796), ('on', 0.005618711616359597), ('so', 0.004965373056317784), ('there', 0.004704037632301058), ('we', 0.004704037632301058)]\n",
      "pMCW:\t ('i', 0.036717627074349925)\n",
      "pMCWx:\t ('i', 0.036717627074349925)\n",
      "pThe:\t ('the', 0.04494969293087678)\n",
      "pComma:\t 0.06768587482033189\n"
     ]
    }
   ],
   "source": [
    "RDD_prob_distr_of_MCWs = prob_distr_of_most_common_words(RDD_word_counter, text_length)\n",
    "print(\"PkMCW:\\t\", RDD_prob_distr_of_MCWs.take(30))\n",
    "\n",
    "prob_the_most_common_word = prob_of_the_most_common_word(RDD_prob_distr_of_MCWs)\n",
    "print(\"pMCW:\\t\", prob_the_most_common_word)\n",
    "\n",
    "prob_the_most_common_word_x = prob_of_the_most_common_word_x(RDD_prob_distr_of_MCWs)\n",
    "print(\"pMCWx:\\t\", prob_the_most_common_word_x)\n",
    "\n",
    "prob_the = prob_of_The(RDD_prob_distr_of_MCWs)\n",
    "print(\"pThe:\\t\", getValue(prob_the))\n",
    "\n",
    "p_comma = prob_of_comma(sentences_data, text_length)\n",
    "print(\"pComma:\\t\", p_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adMCW:\t 27.15357142857143\n",
      "mdMCW:\t 2\n",
      "MdMCW:\t 177\n",
      "adMCWx:\t 27.15357142857143\n",
      "mdMCWx:\t 2\n",
      "MdMCWx:\t 177\n",
      "adThe:\t 22.244897959183675\n",
      "mdThe:\t 2\n",
      "MdThe:\t 115\n",
      "adComma: 14.713733075435202\n",
      "mdComma: 1\n",
      "MdComma: 92\n"
     ]
    }
   ],
   "source": [
    "MCW = prob_the_most_common_word[0]\n",
    "dist_consec_MCW = distance_consec_appear(data, MCW)\n",
    "print(\"adMCW:\\t\", sum(dist_consec_MCW)/len(dist_consec_MCW))\n",
    "print(\"mdMCW:\\t\", min(dist_consec_MCW))\n",
    "print(\"MdMCW:\\t\", max(dist_consec_MCW))\n",
    "\n",
    "MCWx = prob_the_most_common_word_x[0]\n",
    "dist_consec_MCWx = distance_consec_appear(data, MCWx)\n",
    "print(\"adMCWx:\\t\", sum(dist_consec_MCWx)/len(dist_consec_MCWx))\n",
    "print(\"mdMCWx:\\t\", min(dist_consec_MCWx))\n",
    "print(\"MdMCWx:\\t\", max(dist_consec_MCWx))\n",
    "\n",
    "dist_consec_the = distance_consec_appear(data, 'the')\n",
    "print(\"adThe:\\t\", sum(dist_consec_the)/len(dist_consec_the))\n",
    "print(\"mdThe:\\t\", min(dist_consec_the))\n",
    "print(\"MdThe:\\t\", max(dist_consec_the))\n",
    "\n",
    "dist_consec_comma = distance_consec_appear(sentences_data, ',')\n",
    "print(\"adComma:\", sum(dist_consec_comma)/len(dist_consec_comma))\n",
    "print(\"mdComma:\", min(dist_consec_comma))\n",
    "print(\"MdComma:\", max(dist_consec_comma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del file in memoria ... caricamento completato\n",
      "Calcolo delle prime metriche, attendere ... calcolo completato\n",
      "Caricamento del file in memoria ... caricamento completato\n",
      "Calcolo di ulteriori metriche, attendere ... calcolo completato\n",
      "{'vocabulary_size': 1558, 'text_length': 7653, 'V_T': 0.20358029530902913, 'entropy': 8.605861060321123, 'avg_sentence_len': 14.862135922330097, 'max_sentence_len': 84, 'min_sentence_len': 1, 'prob_distr_freq_sen': [(7, 0.06407766990291262), (2, 0.05631067961165048), (9, 0.04660194174757282), (10, 0.04660194174757282), (17, 0.04271844660194175), (12, 0.040776699029126215), (8, 0.038834951456310676), (16, 0.038834951456310676), (1, 0.038834951456310676), (3, 0.038834951456310676), (15, 0.038834951456310676), (6, 0.036893203883495145), (11, 0.03495145631067961), (5, 0.03495145631067961), (13, 0.03300970873786408), (4, 0.031067961165048542), (20, 0.02912621359223301), (25, 0.02330097087378641), (22, 0.02330097087378641), (18, 0.021359223300970873), (14, 0.021359223300970873), (19, 0.019417475728155338), (29, 0.019417475728155338), (23, 0.019417475728155338), (21, 0.017475728155339806), (24, 0.015533980582524271), (26, 0.013592233009708738), (28, 0.013592233009708738), (35, 0.011650485436893204), (30, 0.009708737864077669), (31, 0.009708737864077669), (27, 0.007766990291262136), (41, 0.005825242718446602), (33, 0.005825242718446602), (59, 0.005825242718446602), (37, 0.005825242718446602), (38, 0.005825242718446602), (32, 0.003883495145631068), (34, 0.003883495145631068), (46, 0.003883495145631068), (48, 0.001941747572815534), (56, 0.001941747572815534), (40, 0.001941747572815534), (50, 0.001941747572815534), (42, 0.001941747572815534), (43, 0.001941747572815534), (84, 0.001941747572815534), (61, 0.001941747572815534), (54, 0.001941747572815534), (39, 0.001941747572815534), (63, 0.001941747572815534)], 'prob_most_freq_sen': (7, 0.06407766990291262), 'avg_dist_consec_comma': 14.713733075435202, 'min_dist_consec_comma': 1, 'max_dist_consec_comma': 92}\n"
     ]
    }
   ],
   "source": [
    "metrics = generate_metrics(\"datasets/Anthony Trollope___The O'Conors of Castle Conor from Tales from all Countries.txt\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: confronto prestazioni tra funzioni di Spark e di libreria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSL:\t 84\n",
      "ASL:\t 14.862135922330097\n",
      "mSL:\t 1\n",
      "TIME: \n",
      "14.5108\n",
      "\n",
      "MSL:\t 84\n",
      "ASL:\t 14.862135922330097\n",
      "mSL:\t 1\n",
      "TIME: \n",
      "0.0296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "t = timer()\n",
    "print(\"MSL:\\t\", RDD_sen_lengths.max())\n",
    "print(\"ASL:\\t\", RDD_sen_lengths.mean())\n",
    "print(\"mSL:\\t\", RDD_sen_lengths.min())\n",
    "print(\"TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "t = timer()\n",
    "lengths = RDD_sen_lengths.collect()\n",
    "print(\"MSL:\\t\", max(lengths))\n",
    "print(\"ASL:\\t\", sum(lengths)/len(lengths))\n",
    "print(\"mSL:\\t\", min(lengths))\n",
    "print(\"TIME: \\n{}\\n\".format(round(timer() - t, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
